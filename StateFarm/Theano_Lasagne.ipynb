{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Elemwise{exp,no_inplace}(<TensorType(float64, vector)>)]\n",
      "Looping 1000 times took 1.631349 seconds\n",
      "Result is [ 1.23178032  1.61879341  1.52278065 ...,  2.20771815  2.29967753\n",
      "  1.62323285]\n",
      "Used the cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from theano import function, config, shared, tensor, sandbox\n",
    "import numpy\n",
    "import time\n",
    " \n",
    "vlen = 10 * 30 * 768  # 10 x #cores x # threads per core\n",
    "iters = 1000\n",
    " \n",
    "rng = numpy.random.RandomState(22)\n",
    "x = shared(numpy.asarray(rng.rand(vlen), config.floatX))\n",
    "f = function([], tensor.exp(x))\n",
    "print(f.maker.fgraph.toposort())\n",
    "t0 = time.time()\n",
    "for i in range(iters):\n",
    "    r = f()\n",
    "t1 = time.time()\n",
    "print(\"Looping %d times took %f seconds\" % (iters, t1 - t0))\n",
    "print(\"Result is %s\" % (r,))\n",
    "if numpy.any([isinstance(x.op, tensor.Elemwise) and\n",
    "              ('Gpu' not in type(x.op).__name__)\n",
    "              for x in f.maker.fgraph.toposort()]):\n",
    "    print('Used the cpu')\n",
    "else:\n",
    "    print('Used the gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "import lasagne\n",
    "from lasagne.layers import helper\n",
    "from lasagne.updates import adam\n",
    "from lasagne.nonlinearities import rectify, softmax\n",
    "from lasagne.layers import InputLayer, MaxPool2DLayer, DenseLayer, DropoutLayer, helper\n",
    "from lasagne.layers import Conv2DLayer as ConvLayer\n",
    "\n",
    "import theano\n",
    "from theano import tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Loading data functions\n",
    "'''\n",
    "PIXELS = 24\n",
    "imageSize = PIXELS * PIXELS\n",
    "num_features = imageSize \n",
    "\n",
    "def load_train_cv(encoder):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    print('Read train images')\n",
    "    for j in range(10):\n",
    "        print('Load folder c{}'.format(j))\n",
    "        path = os.path.join('imgs', 'train', '*.jpg')\n",
    "        files = glob.glob(path)\n",
    "        for fl in files:\n",
    "            img = cv2.imread(fl,0)\n",
    "            img = cv2.resize(img, (PIXELS, PIXELS))\n",
    "            #img = img.transpose(2, 0, 1)\n",
    "            img = np.reshape(img, (1, num_features))\n",
    "            X_train.append(img)\n",
    "            y_train.append(j)\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "    y_train = encoder.fit_transform(y_train).astype('int32')\n",
    "\n",
    "    X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.1)\n",
    "\n",
    "    X_train = X_train.reshape(X_train.shape[0], 1, PIXELS, PIXELS).astype('float32') / 255.\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, PIXELS, PIXELS).astype('float32') / 255.\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_test():\n",
    "    print('Read test images')\n",
    "    path = os.path.join('imgs', 'test', '*.jpg')\n",
    "    files = glob.glob(path)\n",
    "    X_test = []\n",
    "    X_test_id = []\n",
    "    total = 0\n",
    "    thr = math.floor(len(files)/10)\n",
    "    for fl in files:\n",
    "        flbase = os.path.basename(fl)\n",
    "        img = cv2.imread(fl,0)\n",
    "        img = cv2.resize(img, (PIXELS, PIXELS))\n",
    "        #img = img.transpose(2, 0, 1)\n",
    "        img = np.reshape(img, (1, num_features))\n",
    "        X_test.append(img)\n",
    "        X_test_id.append(flbase)\n",
    "        total += 1\n",
    "        if total%thr == 0:\n",
    "            print('Read {} images from {}'.format(total, len(files)))\n",
    "\n",
    "    X_test = np.array(X_test)\n",
    "    X_test_id = np.array(X_test_id)\n",
    "\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, PIXELS, PIXELS).astype('float32') / 255.\n",
    "\n",
    "    return X_test, X_test_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Lasagne Model ZFTurboNet and Batch Iterator\n",
    "'''\n",
    "def ZFTurboNet(input_var=None):\n",
    "    l_in = InputLayer(shape=(None, 1, PIXELS, PIXELS), input_var=input_var)\n",
    "\n",
    "    l_conv = ConvLayer(l_in, num_filters=8, filter_size=3, pad=1, nonlinearity=rectify)\n",
    "    l_convb = ConvLayer(l_conv, num_filters=8, filter_size=3, pad=1, nonlinearity=rectify)\n",
    "    l_pool = MaxPool2DLayer(l_convb, pool_size=2) # feature maps 12x12\n",
    "\n",
    "    #l_dropout1 = DropoutLayer(l_pool, p=0.25)\n",
    "    l_hidden = DenseLayer(l_pool, num_units=128, nonlinearity=rectify)\n",
    "    #l_dropout2 = DropoutLayer(l_hidden, p=0.5)\n",
    "\n",
    "    l_out = DenseLayer(l_hidden, num_units=10, nonlinearity=softmax)\n",
    "\n",
    "    return l_out\n",
    "def iterate_minibatches(inputs, targets, batchsize):\n",
    "    assert len(inputs) == len(targets)\n",
    "    indices = np.arange(len(inputs))\n",
    "    np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        yield inputs[excerpt], targets[excerpt]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Set up all theano functions\n",
    "\"\"\"\n",
    "BATCHSIZE = 32\n",
    "LR = 0.001\n",
    "ITERS = 6\n",
    "\n",
    "X = T.tensor4('X')\n",
    "Y = T.ivector('y')\n",
    "\n",
    "# set up theano functions to generate output by feeding data through network, any test outputs should be deterministic\n",
    "output_layer = ZFTurboNet(X)\n",
    "output_train = lasagne.layers.get_output(output_layer)\n",
    "output_test = lasagne.layers.get_output(output_layer, deterministic=True)\n",
    "\n",
    "# set up the loss that we aim to minimize, when using cat cross entropy our Y should be ints not one-hot\n",
    "loss = lasagne.objectives.categorical_crossentropy(output_train, Y)\n",
    "loss = loss.mean()\n",
    "\n",
    "# set up loss functions for validation dataset\n",
    "valid_loss = lasagne.objectives.categorical_crossentropy(output_test, Y)\n",
    "valid_loss = valid_loss.mean()\n",
    "\n",
    "valid_acc = T.mean(T.eq(T.argmax(output_test, axis=1), Y), dtype=theano.config.floatX)\n",
    "\n",
    "# get parameters from network and set up sgd with nesterov momentum to update parameters\n",
    "params = lasagne.layers.get_all_params(output_layer, trainable=True)\n",
    "updates = adam(loss, params, learning_rate=LR)\n",
    "\n",
    "# set up training and prediction functions\n",
    "train_fn = theano.function(inputs=[X,Y], outputs=loss, updates=updates)\n",
    "valid_fn = theano.function(inputs=[X,Y], outputs=[valid_loss, valid_acc])\n",
    "\n",
    "# set up prediction function\n",
    "predict_proba = theano.function(inputs=[X], outputs=output_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read train images\n",
      "Load folder c0\n",
      "Load folder c1\n",
      "Load folder c2\n",
      "Load folder c3\n",
      "Load folder c4\n",
      "Load folder c5\n",
      "Load folder c6\n",
      "Load folder c7\n",
      "Load folder c8\n",
      "Load folder c9\n",
      "('Train shape:', (0, 1, 24, 24), 'Test shape:', (0, 1, 24, 24))\n",
      "Read test images\n",
      "Read 7972 images from 79726\n",
      "Read 15944 images from 79726\n",
      "Read 23916 images from 79726\n",
      "Read 31888 images from 79726\n",
      "Read 39860 images from 79726\n",
      "Read 47832 images from 79726\n",
      "Read 55804 images from 79726\n",
      "Read 63776 images from 79726\n",
      "Read 71748 images from 79726\n",
      "Read 79720 images from 79726\n",
      "('iter:', 0, '| TL:', nan, '| VL:', nan, '| Vacc:', nan, '| Ratio:', nan, '| Time:', 0.0)\n",
      "('iter:', 1, '| TL:', nan, '| VL:', nan, '| Vacc:', nan, '| Ratio:', nan, '| Time:', 0.0)\n",
      "('iter:', 2, '| TL:', nan, '| VL:', nan, '| Vacc:', nan, '| Ratio:', nan, '| Time:', 0.0)\n",
      "('iter:', 3, '| TL:', nan, '| VL:', nan, '| Vacc:', nan, '| Ratio:', nan, '| Time:', 0.0)\n",
      "('iter:', 4, '| TL:', nan, '| VL:', nan, '| Vacc:', nan, '| Ratio:', nan, '| Time:', 0.0)\n",
      "('iter:', 5, '| TL:', nan, '| VL:', nan, '| Vacc:', nan, '| Ratio:', nan, '| Time:', 0.0)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "load training data and start training\n",
    "'''\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# load the training and validation data sets\n",
    "train_X, train_y, valid_X, valid_y, encoder = load_train_cv(encoder)\n",
    "print('Train shape:', train_X.shape, 'Test shape:', valid_X.shape)\n",
    "\n",
    "# load data\n",
    "X_test, X_test_id = load_test()\n",
    "\n",
    "# loop over training functions for however many iterations, print information while training\n",
    "try:\n",
    "    for epoch in range(ITERS):\n",
    "        # do the training\n",
    "        start = time.time()\n",
    "        # training batches\n",
    "        train_loss = []\n",
    "        for batch in iterate_minibatches(train_X, train_y, BATCHSIZE):\n",
    "            inputs, targets = batch\n",
    "            train_loss.append(train_fn(inputs, targets))\n",
    "        train_loss = np.mean(train_loss)\n",
    "        # validation batches\n",
    "        valid_loss = []\n",
    "        valid_acc = []\n",
    "        for batch in iterate_minibatches(valid_X, valid_y, BATCHSIZE):\n",
    "            inputs, targets = batch\n",
    "            valid_eval = valid_fn(inputs, targets)\n",
    "            valid_loss.append(valid_eval[0])\n",
    "            valid_acc.append(valid_eval[1])\n",
    "        valid_loss = np.mean(valid_loss)\n",
    "        valid_acc = np.mean(valid_acc)\n",
    "        # get ratio of TL to VL\n",
    "        ratio = train_loss / valid_loss\n",
    "        end = time.time() - start\n",
    "        # print training details\n",
    "        print('iter:', epoch, '| TL:', np.round(train_loss,decimals=3), '| VL:', np.round(valid_loss, decimals=3), '| Vacc:', np.round(valid_acc, decimals=3), '| Ratio:', np.round(ratio, decimals=2), '| Time:', np.round(end, decimals=1))\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions\n",
      "pred shape\n",
      "(79726, 10)\n",
      "Creating Submission\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Make Submission\n",
    "'''\n",
    "\n",
    "#make predictions\n",
    "print('Making predictions')\n",
    "PRED_BATCH = 2\n",
    "def iterate_pred_minibatches(inputs, batchsize):\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt]\n",
    "\n",
    "predictions = []\n",
    "for pred_batch in iterate_pred_minibatches(X_test, PRED_BATCH):\n",
    "    predictions.extend(predict_proba(pred_batch))\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "\n",
    "print('pred shape')\n",
    "print(predictions.shape)\n",
    "\n",
    "print('Creating Submission')\n",
    "def create_submission(predictions, test_id):\n",
    "    result1 = pd.DataFrame(predictions, columns=['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9'])\n",
    "    result1.loc[:, 'img'] = pd.Series(test_id, index=result1.index)\n",
    "    result1.to_csv('submission_ZFTurboNet.csv', index=False)\n",
    "\n",
    "create_submission(predictions, X_test_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
