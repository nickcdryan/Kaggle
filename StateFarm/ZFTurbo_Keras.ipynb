{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2016)\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import math\n",
    "import pickle\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import time\n",
    "from shutil import copy2\n",
    "import warnings\n",
    "import random\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from keras.models import model_from_json\n",
    "from sklearn.metrics import log_loss\n",
    "from scipy.misc import imread, imresize, imshow\n",
    "\n",
    "use_cache = 1\n",
    "\n",
    "\n",
    "def show_image(im, name='image'):\n",
    "    cv2.imshow(name, im)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# color_type = 1 - gray\n",
    "# color_type = 3 - RGB\n",
    "def get_im_cv2(path, img_rows, img_cols, color_type=1):\n",
    "    # Load as grayscale\n",
    "    if color_type == 1:\n",
    "        img = cv2.imread(path, 0)\n",
    "    elif color_type == 3:\n",
    "        img = cv2.imread(path)\n",
    "    # Reduce size\n",
    "    resized = cv2.resize(img, (img_cols, img_rows), cv2.INTER_LINEAR)\n",
    "    return resized\n",
    "\n",
    "\n",
    "def get_im_cv2_mod(path, img_rows, img_cols, color_type=1):\n",
    "    # Load as grayscale\n",
    "    if color_type == 1:\n",
    "        img = cv2.imread(path, 0)\n",
    "    else:\n",
    "        img = cv2.imread(path)\n",
    "    # Reduce size\n",
    "    rotate = random.uniform(-10, 10)\n",
    "    M = cv2.getRotationMatrix2D((img.shape[1]/2, img.shape[0]/2), rotate, 1)\n",
    "    img = cv2.warpAffine(img, M, (img.shape[1], img.shape[0]))\n",
    "    resized = cv2.resize(img, (img_cols, img_rows), cv2.INTER_LINEAR)\n",
    "    return resized\n",
    "\n",
    "\n",
    "def get_driver_data():\n",
    "    dr = dict()\n",
    "    clss = dict()\n",
    "    path = os.path.join('driver_imgs_list.csv')\n",
    "    print('Read drivers data')\n",
    "    f = open(path, 'r')\n",
    "    line = f.readline()\n",
    "    while (1):\n",
    "        line = f.readline()\n",
    "        if line == '':\n",
    "            break\n",
    "        arr = line.strip().split(',')\n",
    "        dr[arr[2]] = arr[0]\n",
    "        if arr[0] not in clss.keys():\n",
    "            clss[arr[0]] = [(arr[1], arr[2])]\n",
    "        else:\n",
    "            clss[arr[0]].append((arr[1], arr[2]))\n",
    "    f.close()\n",
    "    return dr, clss\n",
    "\n",
    "\n",
    "def load_train(img_rows, img_cols, color_type=1):\n",
    "    X_train = []\n",
    "    X_train_id = []\n",
    "    y_train = []\n",
    "    driver_id = []\n",
    "    start_time = time.time()\n",
    "    driver_data, dr_class = get_driver_data()\n",
    "\n",
    "    print('Read train images')\n",
    "    for j in range(10):\n",
    "        print('Load folder c{}'.format(j))\n",
    "        path = os.path.join('imgs', 'train', 'c' + str(j), '*.jpg')\n",
    "        files = glob.glob(path)\n",
    "        for fl in files:\n",
    "            flbase = os.path.basename(fl)\n",
    "            # img = get_im_cv2(fl, img_rows, img_cols, color_type)\n",
    "            img = get_im_cv2_mod(fl, img_rows, img_cols, color_type)\n",
    "            X_train.append(img)\n",
    "            X_train_id.append(flbase)\n",
    "            y_train.append(j)\n",
    "            driver_id.append(driver_data[flbase])\n",
    "\n",
    "    print('Read train data time: {} seconds'.format(round(time.time() - start_time, 2)))\n",
    "    unique_drivers = sorted(list(set(driver_id)))\n",
    "    print('Unique drivers: {}'.format(len(unique_drivers)))\n",
    "    print(unique_drivers)\n",
    "    return X_train, y_train, X_train_id, driver_id, unique_drivers\n",
    "\n",
    "\n",
    "def load_test(img_rows, img_cols, color_type=1):\n",
    "    print('Read test images')\n",
    "    path = os.path.join('imgs', 'test', '*.jpg')\n",
    "    files = glob.glob(path)\n",
    "    X_test = []\n",
    "    X_test_id = []\n",
    "    total = 0\n",
    "    start_time = time.time()\n",
    "    thr = math.floor(len(files)/10)\n",
    "    for fl in files:\n",
    "        flbase = os.path.basename(fl)\n",
    "        # img = get_im_cv2(fl, img_rows, img_cols, color_type)\n",
    "        img = get_im_cv2_mod(fl, img_rows, img_cols, color_type)\n",
    "        X_test.append(img)\n",
    "        X_test_id.append(flbase)\n",
    "        total += 1\n",
    "        if total%thr == 0:\n",
    "            print('Read {} images from {}'.format(total, len(files)))\n",
    "\n",
    "    print('Read test data time: {} seconds'.format(round(time.time() - start_time, 2)))\n",
    "    return X_test, X_test_id\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read drivers data\n",
      "Read train images\n",
      "Load folder c0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "dst is not a numpy array, neither a scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9243c1a6ee01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m     \u001b[0mrun_cross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-9243c1a6ee01>\u001b[0m in \u001b[0;36mrun_cross_validation\u001b[0;34m(nfolds)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0mrestore_from_last_checkpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m     \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdriver_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique_drivers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_and_normalize_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor_type_global\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m     \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_and_normalize_test_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor_type_global\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model_v1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor_type_global\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-9243c1a6ee01>\u001b[0m in \u001b[0;36mread_and_normalize_train_data\u001b[0;34m(img_rows, img_cols, color_type)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mcache_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cache'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_r_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_rows\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_c_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_cols\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_t_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_rotated.dat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0muse_cache\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdriver_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique_drivers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mcache_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdriver_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique_drivers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-4ec38639445d>\u001b[0m in \u001b[0;36mload_train\u001b[0;34m(img_rows, img_cols, color_type)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mflbase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;31m# img = get_im_cv2(fl, img_rows, img_cols, color_type)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_im_cv2_mod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m             \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mX_train_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflbase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-4ec38639445d>\u001b[0m in \u001b[0;36mget_im_cv2_mod\u001b[0;34m(path, img_rows, img_cols, color_type)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetRotationMatrix2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrotate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarpAffine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mresized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_rows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTER_LINEAR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: dst is not a numpy array, neither a scalar"
     ]
    }
   ],
   "source": [
    "def cache_data(data, path):\n",
    "    if os.path.isdir(os.path.dirname(path)):\n",
    "        file = open(path, 'wb')\n",
    "        pickle.dump(data, file)\n",
    "        file.close()\n",
    "    else:\n",
    "        print('Directory doesnt exists')\n",
    "\n",
    "\n",
    "def restore_data(path):\n",
    "    data = dict()\n",
    "    if os.path.isfile(path):\n",
    "        file = open(path, 'rb')\n",
    "        data = pickle.load(file)\n",
    "    return data\n",
    "\n",
    "\n",
    "def save_model(model, arch_path, weights_path):\n",
    "    json_string = model.to_json()\n",
    "    if not os.path.isdir('cache'):\n",
    "        os.mkdir('cache')\n",
    "    open(arch_path, 'w').write(json_string)\n",
    "    model.save_weights(weights_path, overwrite=True)\n",
    "\n",
    "\n",
    "def read_model(arch_path, weights_path):\n",
    "    model = model_from_json(open(arch_path).read())\n",
    "    model.load_weights(weights_path)\n",
    "    return model\n",
    "\n",
    "\n",
    "def split_validation_set(train, target, test_size):\n",
    "    random_state = 51\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=test_size, random_state=random_state)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def create_submission(predictions, test_id, info):\n",
    "    result1 = pd.DataFrame(predictions, columns=['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9'])\n",
    "    result1.loc[:, 'img'] = pd.Series(test_id, index=result1.index)\n",
    "    now = datetime.datetime.now()\n",
    "    if not os.path.isdir('subm'):\n",
    "        os.mkdir('subm')\n",
    "    suffix = info + '_' + str(now.strftime(\"%Y-%m-%d-%H-%M\"))\n",
    "    sub_file = os.path.join('subm', 'submission_' + suffix + '.csv')\n",
    "    result1.to_csv(sub_file, index=False)\n",
    "\n",
    "\n",
    "def save_useful_data(predictions_valid, valid_ids, model, info):\n",
    "    result1 = pd.DataFrame(predictions_valid, columns=['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9'])\n",
    "    result1.loc[:, 'img'] = pd.Series(valid_ids, index=result1.index)\n",
    "    now = datetime.datetime.now()\n",
    "    if not os.path.isdir(os.path.join('subm', 'data')):\n",
    "        os.mkdir(os.path.join('subm', 'data'))\n",
    "    suffix = info + '_' + str(now.strftime(\"%Y-%m-%d-%H-%M\"))\n",
    "    # Save predictions\n",
    "    pred_file = os.path.join('subm', 'data', 's_' + suffix + '_train_predictions.csv')\n",
    "    result1.to_csv(pred_file, index=False)\n",
    "    # Save model\n",
    "    json_string = model.to_json()\n",
    "    model_file = os.path.join('subm', 'data', 's_' + suffix + '_model.json')\n",
    "    open(model_file, 'w').write(json_string)\n",
    "    # Save code\n",
    "    cur_code = os.path.realpath(__file__)\n",
    "    code_file = os.path.join('subm', 'data', 's_' + suffix + '_code.py')\n",
    "    copy2(cur_code, code_file)\n",
    "\n",
    "\n",
    "def read_and_normalize_train_data(img_rows, img_cols, color_type=1):\n",
    "    cache_path = os.path.join('cache', 'train_r_' + str(img_rows) + '_c_' + str(img_cols) + '_t_' + str(color_type) + '_rotated.dat')\n",
    "    if not os.path.isfile(cache_path) or use_cache == 0:\n",
    "        train_data, train_target, train_id, driver_id, unique_drivers = load_train(img_rows, img_cols, color_type)\n",
    "        cache_data((train_data, train_target, train_id, driver_id, unique_drivers), cache_path)\n",
    "    else:\n",
    "        print('Restore train from cache!')\n",
    "        (train_data, train_target, train_id, driver_id, unique_drivers) = restore_data(cache_path)\n",
    "\n",
    "    train_data = np.array(train_data, dtype=np.uint8)\n",
    "    train_target = np.array(train_target, dtype=np.uint8)\n",
    "\n",
    "    if color_type == 1:\n",
    "        train_data = train_data.reshape(train_data.shape[0], 1, img_rows, img_cols)\n",
    "    else:\n",
    "        train_data = train_data.transpose((0, 3, 1, 2))\n",
    "\n",
    "    train_target = np_utils.to_categorical(train_target, 10)\n",
    "    train_data = train_data.astype('float32')\n",
    "    train_data /= 255\n",
    "    print('Train shape:', train_data.shape)\n",
    "    print(train_data.shape[0], 'train samples')\n",
    "    return train_data, train_target, train_id, driver_id, unique_drivers\n",
    "\n",
    "\n",
    "def read_and_normalize_test_data(img_rows, img_cols, color_type=1):\n",
    "    cache_path = os.path.join('cache', 'test_r_' + str(img_rows) + '_c_' + str(img_cols) + '_t_' + str(color_type) + '_rotated.dat')\n",
    "    if not os.path.isfile(cache_path) or use_cache == 0:\n",
    "        test_data, test_id = load_test(img_rows, img_cols, color_type)\n",
    "        cache_data((test_data, test_id), cache_path)\n",
    "    else:\n",
    "        print('Restore test from cache!')\n",
    "        (test_data, test_id) = restore_data(cache_path)\n",
    "\n",
    "    test_data = np.array(test_data, dtype=np.uint8)\n",
    "    if color_type == 1:\n",
    "        test_data = test_data.reshape(test_data.shape[0], 1, img_rows, img_cols)\n",
    "    else:\n",
    "        test_data = test_data.transpose((0, 3, 1, 2))\n",
    "    # test_data = test_data.swapaxes(3, 1)\n",
    "    test_data = test_data.astype('float32')\n",
    "    test_data /= 255\n",
    "    print('Test shape:', test_data.shape)\n",
    "    print(test_data.shape[0], 'test samples')\n",
    "    return test_data, test_id\n",
    "\n",
    "\n",
    "def merge_several_folds_mean(data, nfolds):\n",
    "    a = np.array(data[0])\n",
    "    for i in range(1, nfolds):\n",
    "        a += np.array(data[i])\n",
    "    a /= nfolds\n",
    "    return a.tolist()\n",
    "\n",
    "\n",
    "def merge_several_folds_geom(data, nfolds):\n",
    "    a = np.array(data[0])\n",
    "    for i in range(1, nfolds):\n",
    "        a *= np.array(data[i])\n",
    "    a = np.power(a, 1/nfolds)\n",
    "    return a.tolist()\n",
    "\n",
    "\n",
    "def copy_selected_drivers(train_data, train_target, driver_id, driver_list):\n",
    "    data = []\n",
    "    target = []\n",
    "    index = []\n",
    "    for i in range(len(driver_id)):\n",
    "        if driver_id[i] in driver_list:\n",
    "            data.append(train_data[i])\n",
    "            target.append(train_target[i])\n",
    "            index.append(i)\n",
    "    data = np.array(data)\n",
    "    target = np.array(target)\n",
    "    index = np.array(index)\n",
    "    return data, target, index\n",
    "\n",
    "\n",
    "def create_model_v1(img_rows, img_cols, color_type=1):\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(32, 3, 3, border_mode='same', init='he_normal',\n",
    "                            input_shape=(color_type, img_rows, img_cols)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Convolution2D(64, 3, 3, border_mode='same', init='he_normal'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Convolution2D(128, 3, 3, border_mode='same', init='he_normal'))\n",
    "    model.add(MaxPooling2D(pool_size=(8, 8)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(Adam(lr=1e-3), loss='categorical_crossentropy')\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_validation_predictions(train_data, predictions_valid):\n",
    "    pv = []\n",
    "    for i in range(len(train_data)):\n",
    "        pv.append(predictions_valid[i])\n",
    "    return pv\n",
    "\n",
    "\n",
    "def run_cross_validation(nfolds=10):\n",
    "    # input image dimensions\n",
    "    img_rows, img_cols = 64, 64\n",
    "    # color type: 1 - grey, 3 - rgb\n",
    "    color_type_global = 1\n",
    "    batch_size = 16\n",
    "    nb_epoch = 50\n",
    "    random_state = 51\n",
    "    restore_from_last_checkpoint = 0\n",
    "\n",
    "    train_data, train_target, train_id, driver_id, unique_drivers = read_and_normalize_train_data(img_rows, img_cols, color_type_global)\n",
    "    test_data, test_id = read_and_normalize_test_data(img_rows, img_cols, color_type_global)\n",
    "    model = create_model_v1(img_rows, img_cols, color_type_global)\n",
    "\n",
    "    yfull_train = dict()\n",
    "    yfull_test = []\n",
    "    kf = KFold(len(unique_drivers), n_folds=nfolds, shuffle=True, random_state=random_state)\n",
    "    num_fold = 0\n",
    "    sum_score = 0\n",
    "    for train_drivers, test_drivers in kf:\n",
    "        unique_list_train = [unique_drivers[i] for i in train_drivers]\n",
    "        X_train, Y_train, train_index = copy_selected_drivers(train_data, train_target, driver_id, unique_list_train)\n",
    "        unique_list_valid = [unique_drivers[i] for i in test_drivers]\n",
    "        X_valid, Y_valid, test_index = copy_selected_drivers(train_data, train_target, driver_id, unique_list_valid)\n",
    "\n",
    "        num_fold += 1\n",
    "        print('Start KFold number {} from {}'.format(num_fold, nfolds))\n",
    "        print('Split train: ', len(X_train), len(Y_train))\n",
    "        print('Split valid: ', len(X_valid), len(Y_valid))\n",
    "        print('Train drivers: ', unique_list_train)\n",
    "        print('Test drivers: ', unique_list_valid)\n",
    "\n",
    "        kfold_weights_path = os.path.join('cache', 'weights_kfold_' + str(num_fold) + '.h5')\n",
    "        if not os.path.isfile(kfold_weights_path) or restore_from_last_checkpoint == 0:\n",
    "            callbacks = [\n",
    "                EarlyStopping(monitor='val_loss', patience=1, verbose=0),\n",
    "                ModelCheckpoint(kfold_weights_path, monitor='val_loss', save_best_only=True, verbose=0),\n",
    "            ]\n",
    "            model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                  shuffle=True, verbose=1, validation_data=(X_valid, Y_valid),\n",
    "                  callbacks=callbacks)\n",
    "        if os.path.isfile(kfold_weights_path):\n",
    "            model.load_weights(kfold_weights_path)\n",
    "\n",
    "        # score = model.evaluate(X_valid, Y_valid, show_accuracy=True, verbose=0)\n",
    "        # print('Score log_loss: ', score[0])\n",
    "\n",
    "        predictions_valid = model.predict(X_valid, batch_size=batch_size, verbose=1)\n",
    "        score = log_loss(Y_valid, predictions_valid)\n",
    "        print('Score log_loss: ', score)\n",
    "        sum_score += score*len(test_index)\n",
    "\n",
    "        # Store valid predictions\n",
    "        for i in range(len(test_index)):\n",
    "            yfull_train[test_index[i]] = predictions_valid[i]\n",
    "\n",
    "        # Store test predictions\n",
    "        test_prediction = model.predict(test_data, batch_size=batch_size, verbose=1)\n",
    "        yfull_test.append(test_prediction)\n",
    "\n",
    "    score = sum_score/len(train_data)\n",
    "    print(\"Log_loss train independent avg: \", score)\n",
    "\n",
    "    predictions_valid = get_validation_predictions(train_data, yfull_train)\n",
    "    score1 = log_loss(train_target, predictions_valid)\n",
    "    if abs(score1 - score) > 0.0001:\n",
    "        print('Check error: {} != {}'.format(score, score1))\n",
    "\n",
    "    print('Final log_loss: {}, rows: {} cols: {} nfolds: {} epoch: {}'.format(score, img_rows, img_cols, nfolds, nb_epoch))\n",
    "    info_string = 'loss_' + str(score) \\\n",
    "                    + '_r_' + str(img_rows) \\\n",
    "                    + '_c_' + str(img_cols) \\\n",
    "                    + '_folds_' + str(nfolds) \\\n",
    "                    + '_ep_' + str(nb_epoch)\n",
    "\n",
    "    test_res = merge_several_folds_mean(yfull_test, nfolds)\n",
    "    # test_res = merge_several_folds_geom(yfull_test, nfolds)\n",
    "    create_submission(test_res, test_id, info_string)\n",
    "    save_useful_data(predictions_valid, train_id, model, info_string)\n",
    "\n",
    "\n",
    "def run_single():\n",
    "    # input image dimensions\n",
    "    img_rows, img_cols = 64, 64\n",
    "    color_type_global = 1\n",
    "    batch_size = 32\n",
    "    nb_epoch = 50\n",
    "    random_state = 51\n",
    "\n",
    "    train_data, train_target, train_id, driver_id, unique_drivers = read_and_normalize_train_data(img_rows, img_cols, color_type_global)\n",
    "    test_data, test_id = read_and_normalize_test_data(img_rows, img_cols, color_type_global)\n",
    "\n",
    "    yfull_test = []\n",
    "    unique_list_train = ['p002', 'p012', 'p014', 'p015', 'p016', 'p021', 'p022', 'p035', 'p041', 'p042', 'p045', 'p047', 'p049', 'p050', 'p051', 'p052', 'p056', 'p061', 'p064', 'p066', 'p075', 'p081']\n",
    "    X_train, Y_train, train_index = copy_selected_drivers(train_data, train_target, driver_id, unique_list_train)\n",
    "    unique_list_valid = ['p024', 'p026', 'p039', 'p072']\n",
    "    X_valid, Y_valid, test_index = copy_selected_drivers(train_data, train_target, driver_id, unique_list_valid)\n",
    "\n",
    "    print('Start Single Run')\n",
    "    print('Split train: ', len(X_train))\n",
    "    print('Split valid: ', len(X_valid))\n",
    "    print('Train drivers: ', unique_list_train)\n",
    "    print('Valid drivers: ', unique_list_valid)\n",
    "\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=2, verbose=0),\n",
    "    ]\n",
    "    model = create_model_v1(img_rows, img_cols, color_type_global)\n",
    "    model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "              shuffle=True, verbose=1, validation_data=(X_valid, Y_valid),\n",
    "              callbacks=callbacks)\n",
    "\n",
    "    # score = model.evaluate(X_valid, Y_valid, show_accuracy=True, verbose=0)\n",
    "    # print('Score log_loss: ', score[0])\n",
    "\n",
    "    predictions_valid = model.predict(X_valid, batch_size=batch_size, verbose=1)\n",
    "    score = log_loss(Y_valid, predictions_valid)\n",
    "    print('Score log_loss: ', score)\n",
    "\n",
    "    # Store test predictions\n",
    "    test_prediction = model.predict(test_data, batch_size=batch_size, verbose=1)\n",
    "    yfull_test.append(test_prediction)\n",
    "\n",
    "    print('Final log_loss: {}, rows: {} cols: {} epoch: {}'.format(score, img_rows, img_cols, nb_epoch))\n",
    "    info_string = 'loss_' + str(score) \\\n",
    "                    + '_r_' + str(img_rows) \\\n",
    "                    + '_c_' + str(img_cols) \\\n",
    "                    + '_ep_' + str(nb_epoch)\n",
    "\n",
    "    full_pred = model.predict(train_data, batch_size=batch_size, verbose=1)\n",
    "    score = log_loss(train_target, full_pred)\n",
    "    print('Full score log_loss: ', score)\n",
    "\n",
    "    test_res = merge_several_folds_mean(yfull_test, 1)\n",
    "    create_submission(test_res, test_id, info_string)\n",
    "    save_useful_data(full_pred, train_id, model, info_string)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_cross_validation(13)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
